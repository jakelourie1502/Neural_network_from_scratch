{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd03140f74d8e1d28421ccd34109d4bdb636d1200d25e7dadfa8785402db4096f74",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Creating a SoftMax Neural Net from Scratch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plot \n",
    "import seaborn \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, y_train, x_test, y_test = x_train.astype('int'), y_train.astype('int'), x_test.astype('int'), y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Net_Softmax():\n",
    "    \n",
    "    #Initialise with the input shape and output shape\n",
    "    def __init__(self, input_features, output_features):\n",
    "        self.input_dimension = input_features\n",
    "        self.output_dimensions = output_features\n",
    "        \n",
    "        #Initialise the dictionaries here so we can't accidentally store values we don't need if we change the input sizes.\n",
    "        self.forward_prop_dict = {}\n",
    "        self.back_prop_dict = {}\n",
    "\n",
    "    def create_layers(array_of_mid_layer_sizes, activation_functions = 'relu'):\n",
    "        self.layer_sizes = array_of_mid_layer_sizes\n",
    "        \n",
    "        #for layer type, we can either set all layers to one type or have an array of layers passed\n",
    "        if activation_functions in ('relu', 'sigmoid')::\n",
    "            self.layer_types = [activation_functions] * len(array_of_mid_layer_sizes)\n",
    "        else:\n",
    "            self.layer_types = activation_functions\n",
    "\n",
    "    def init_weights(self,array_of_mid_layer_sizes, array_of_mid_layer_activations, init_method='gaussian'):\n",
    "        self.weights = {}\n",
    "        all_layers = [self.input_dimension] + array_of_mid_layer_sizes + [self.output_dimensions] #\n",
    "        \n",
    "        for layer in range(len(all_layers) - 1):\n",
    "            output_dim = all_layers[layer+1]\n",
    "            input_dim = all_layers[layer] \n",
    "\n",
    "            #Gaussian, Xavier or He weights initialisation methods below (COULD DO THIS IN HELPER FUNCTIONS AND THEN CALL THE RELEVANT FUNCTION)\n",
    "            if init_method == 'gaussian': \n",
    "                self.weights[f'W{layer+1}'] = np.random.randn(output_dim,input_dim)\n",
    "                self.weights[f'B{layer+1}'] = np.random.randn(output_dim,1)\n",
    "            \n",
    "            if init_method == 'xavier': \n",
    "                self.weights[f'W{layer+1}'] = np.random.uniform(-np.sqrt(6)/(np.sqrt(output_dim+input_dim)),np.sqrt(6)/(np.sqrt(output_dim+input_dim)),[output_dim,input_dim])\n",
    "                self.weights[f'B{layer+1}'] = np.random.uniform(-np.sqrt(6)/(np.sqrt(output_dim+1)),np.sqrt(6)/(np.sqrt(output_dim+1)),[output_dim,1])\n",
    "\n",
    "            ##ADD IN THE 'HE' for relu\n",
    "        return self.dict_of_weights\n",
    "    \n",
    "    def forward_prop(self, x_flat, one_hot_y, regularisation = False, gradient_smoother = False):\n",
    "        self.forward_prop_dict['A0'] = x_flat #This allows us include x_train in the loop.\n",
    "        layers = int(len(self.weights.keys())/2)\n",
    "\n",
    "        #First, do all the layers up to thte softmax layer ,which has a different activation function.\n",
    "        for key, activation_function in zip(range(1,layers),self.layer_types): #starts at 1, goes up to the last hidden layer\n",
    "            #call the relevant function\n",
    "            activation = locals()[f'_{activation_function}_fp']\n",
    "            self.forward_prop_dict[f'Z{key}'], self.forward_prop_dict[f'A{key}'] = activation(self.forward_prop_dict[f'A{key-1}'], self.weights[f'W{key}'], self.weights[f'B{key}'])\n",
    "\n",
    "        #Then do the final layer\n",
    "        #then add cost function\n",
    "    '''end of function'''\n",
    "\n",
    "    \n",
    "    #Helper functions defining sigmoid, relu and softmax activation functions.\n",
    "    def _sigmoid(x):\n",
    "        return (1/(1+np.exp(-x)))\n",
    "    def _sigmoid_fp(prev_a, W, B):\n",
    "        Z = W @ prev_a + B\n",
    "        return Z, _sigmoid(Z)\n",
    "    \n",
    "    def _relu(x):\n",
    "        return(max(0,x))\n",
    "    def _relu_fp(prev_a, W, B):\n",
    "        Z = W @ prev_a + B\n",
    "        return Z, _relu(Z)\n",
    "\n",
    "    def _last_leg_softmax(previous_a, W, B):\n",
    "        Z = W @ previous_a + B\n",
    "        A = np.exp(Z)\n",
    "        A = A / np.sum(A,axis=0) \n",
    "        return Z, A   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_layers1(array_of_mid_layer_sizes, activation_functions = 'relu'):\n",
    "        layer_sizes = array_of_mid_layer_sizes\n",
    "        \n",
    "        #for layer type, we can either set all layers to one type or have an array of layers passed\n",
    "        if activation_functions in ('relu', 'sigmoid'):\n",
    "            layer_types = [activation_functions] * len(array_of_mid_layer_sizes)\n",
    "        else:\n",
    "            layer_types = activation_functions\n",
    "        return layer_sizes, layer_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_a = locals()[\"create_layers1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([1, 2, 3], ['relu', 'relu', 'relu'])"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "function_a([1,2,3],'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 relu\n1 relu\n2 relu\n"
     ]
    }
   ],
   "source": [
    "layers_test = create_layers1([1,2,3],'relu')\n",
    "for key, layers in zip(range(3),layers_test[1]):\n",
    "    print(key,layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}