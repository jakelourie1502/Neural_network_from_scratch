{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd03140f74d8e1d28421ccd34109d4bdb636d1200d25e7dadfa8785402db4096f74",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Creating a SoftMax Neural Net from Scratch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plot \n",
    "import seaborn \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, y_train, x_test, y_test = x_train.astype('int'), y_train.astype('int'), x_test.astype('int'), y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Net_Softmax():\n",
    "    \n",
    "    #Initialise with the input shape and output shape\n",
    "    def __init__(self, input_features, output_features):\n",
    "        self.input_dimension = input_features\n",
    "        self.output_dimensions = output_features\n",
    "\n",
    "    def init_weights(self,array_of_mid_layer_sizes, array_of_mid_layer_activations, init_method='gaussian'):\n",
    "        self.dict_of_weights = {}\n",
    "        all_layers = [self.input_dimension] + array_of_mid_layer_sizes + [self.output_dimensions] #\n",
    "        \n",
    "        for layer in range(len(array_of_mid_layer_sizes) - 1):\n",
    "            output_dim = all_layers[layer+1]\n",
    "            input_dim = all_layers[layer] \n",
    "\n",
    "            #Gaussian, Xavier or He weights initialisation methods below\n",
    "            if init_method == 'gaussian': \n",
    "                self.dict_of_weights[f'W{layer+1}'] = np.random.randn(output_dim,input_dim)\n",
    "                self.dict_of_weights[f'B{layer+1}'] = np.random.randn(output_dim,1)\n",
    "            \n",
    "            if init_method == 'xavier': \n",
    "                self.dict_of_weights[f'W{layer+1}'] = np.random.uniform(-np.sqrt(6)/(np.sqrt(output_dim+input_dim)),np.sqrt(6)/(np.sqrt(output_dim+input_dim)),[output_dim,input_dim])\n",
    "                self.dict_of_weights[f'B{layer+1}'] = np.random.uniform(-np.sqrt(6)/(np.sqrt(output_dim+1)),np.sqrt(6)/(np.sqrt(output_dim+1)),[output_dim,1])\n",
    "\n",
    "            ##ADD IN THE 'HE' for relu\n",
    "        return self.dict_of_weights\n",
    "    \n",
    "    def forward_prop(self, x_flattened, one_hot_y, dict_of_prev_values ={}):\n",
    "\n",
    "    #Helper functions defining sigmod and relu activation functions.\n",
    "    def _sigmoid(x):\n",
    "        return (1/(1+np.exp(-x)))\n",
    "    def _sigmoid_fp(prev_a, W, B):\n",
    "        Z = W @ prev_a + B\n",
    "        return Z, _sigmoid(Z)\n",
    "    \n",
    "    def _relu(x):\n",
    "        return(max(0,x))\n",
    "    def _relu_fp(prev_a, W, B):\n",
    "        Z = W @ prev_a + B\n",
    "        return Z, _relu(Z)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.14842238536580632"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "np.var(nn_1.dict_of_weights['W1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}