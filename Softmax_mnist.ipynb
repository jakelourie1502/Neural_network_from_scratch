{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd03140f74d8e1d28421ccd34109d4bdb636d1200d25e7dadfa8785402db4096f74",
   "display_name": "Python 3.8.5  ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "3140f74d8e1d28421ccd34109d4bdb636d1200d25e7dadfa8785402db4096f74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Creating a SoftMax Neural Net from Scratch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plot \n",
    "import seaborn \n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, y_train, x_test, y_test = x_train.astype('int'), y_train.astype('int'), x_test.astype('int'), y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation section\n",
    "\n",
    "def create_one_hot_train_and_test(y_train, y_test):\n",
    "    one_hot = OneHotEncoder(sparse = False)\n",
    "    one_hot_y_train= one_hot.fit_transform(y_train.reshape(-1,1)).T ###currently doing it on two classes\n",
    "    one_hot_y_test = one_hot.fit_transform(y_test.reshape(-1,1)).T\n",
    "    return one_hot_y_train, one_hot_y_test\n",
    "\n",
    "def flatten_and_normalise_x_values(x_train, x_test):\n",
    "    x_train_flat = x_train.reshape((x_train.shape[0],-1)).T / 255\n",
    "    x_test_flat = x_test.reshape((x_test.shape[0],-1)).T / 255\n",
    "    return x_train_flat, x_test_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_flat, x_test_flat = flatten_and_normalise_x_values(x_train,x_test)\n",
    "one_hot_y_train, one_hot_y_test = create_one_hot_train_and_test(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Net_Softmax():\n",
    "    \n",
    "    #Initialise with the input shape and output shape\n",
    "    def __init__(self, input_features, output_features):\n",
    "        self.input_dimension = input_features\n",
    "        self.output_dimensions = output_features\n",
    "        #Initialise the dictionaries here so we can't accidentally store values we don't need if we change the input sizes.\n",
    "        self.fp_dict = {}\n",
    "        self.bp_dict = {}\n",
    "\n",
    "    def create_layers(self,array_of_mid_layer_sizes, activation_functions = 'relu'):\n",
    "        self.mid_layer_sizes = array_of_mid_layer_sizes        \n",
    "        #for layer type, we can either set all layers to one type or have an array of layers passed\n",
    "        if activation_functions in ('relu', 'sigmoid'):\n",
    "            self.mid_layer_types = [activation_functions] * len(array_of_mid_layer_sizes)\n",
    "        else:\n",
    "            self.mid_layer_types = activation_functions\n",
    "        self.fp_rounds = len(array_of_mid_layer_sizes) + 1 #Define this here as will be used a lot later and creates one terminology\n",
    "        self.all_layers = [self.input_dimension] + self.mid_layer_sizes + [self.output_dimensions] #\n",
    "\n",
    "    def init_weights(self, init_methods='gaussian'):\n",
    "        self.weights = {}\n",
    "        \n",
    "        #Allow either a list of init_methods or one init method for all layers\n",
    "        if type(init_methods) == str:\n",
    "            init_methods = [init_methods] * self.fp_rounds #This creates a list of layers-1 weight initialisation strings.\n",
    "        \n",
    "        #this iterates through the size of layer and the init methods\n",
    "        for layer, init_methods in zip(range(self.fp_rounds),init_methods):\n",
    "            output_dim = self.all_layers[layer+1]\n",
    "            input_dim = self.all_layers[layer] \n",
    "\n",
    "            #Use locals to call the reelvant function\n",
    "            self.weights[f'W{layer+1}'], self.weights[f'B{layer+1}'] = globals()[f'_{init_methods}_init'](output_dim, input_dim)\n",
    "        return self.weights\n",
    "    \n",
    "    def forward_prop(self, x_flat, one_hot_y, regularisation = False, gradient_smoother = False):\n",
    "        self.fp_dict['A0'], self.fp_dict['Z0'] = x_flat,0 #helpful for iteratting through a's and Z's in backprop\n",
    "        fp_rounds = self.fp_rounds\n",
    "\n",
    "        #First, do all the layers up to thte softmax layer ,which has a different activation function.\n",
    "        for key, activation_function in zip(range(1,fp_rounds),self.mid_layer_types): #starts at 1, goes up to the last hidden layer\n",
    "            #call the relevant function\n",
    "            activation = globals()[f'_{activation_function}_fp']\n",
    "            self.fp_dict[f'Z{key}'], self.fp_dict[f'A{key}'] = _sigmoid_fp(self.fp_dict[f'A{key-1}'], self.weights[f'W{key}'], self.weights[f'B{key}'])\n",
    "\n",
    "        #Then do the final layer and calculate cost\n",
    "        self.fp_dict[f'Z{fp_rounds}'], self.fp_dict[f'A{fp_rounds}'] = _last_leg_softmax(self.fp_dict[f'A{fp_rounds-1}'], self.weights[f'W{fp_rounds}'], self.weights[f'B{fp_rounds}']) \n",
    "        cost = -np.sum(one_hot_y * np.log(self.fp_dict[f'A{fp_rounds}'])) / one_hot_y.shape[1] #divides by training examples\n",
    "        return self.fp_dict, cost\n",
    "\n",
    "    def back_prop(self, x_flat, one_hot_y):\n",
    "        ZA_dict = {} #this is a temporary dict to store values that are needed at later stages of back prop.\n",
    "        ZA_dict['Z0'] = {} #Just useful for iterations.\n",
    "        #The first dC_dZ is the classic Softmax function, Ã¿ - y\n",
    "        ZA_dict[f'dC_dZ{self.fp_rounds}'] = self.fp_dict[f'A{self.fp_rounds}'] - one_hot_y\n",
    "        \n",
    "        #Then we need to iterate back across all layers, getting the relevant dw's for grad descent and the dZs for the previous layer        \n",
    "        final_layer_code = 'no' #this is to solve the issue of the last layer not having Z values\n",
    "        for key in reversed(range(1,self.fp_rounds+1)): #e.g. for a network with 5 layers (and 5 Ws, 5 Bs), this will iterate 5,4,3,2,1 and on 1 it will have final layer code of 'yes'\n",
    "            if key == 1:\n",
    "                final_layer_code = 'yes'\n",
    "            self.bp_dict[f'dW{key}'], self.bp_dict[f'dB{key}'], ZA_dict[f'dC_dZ{key-1}'] = back_prop_from_z_to_z_sigmoid(ZA_dict[f'dC_dZ{key}'], final_layer_code, self.fp_dict[f'Z{key-1}'], self.fp_dict[f'A{key-1}'], self.weights[f'W{key}'])\n",
    "        return self.bp_dict\n",
    "\n",
    "'''Helper functions - outside of class so they can be called with globals'''\n",
    "\n",
    "#weights initialisati\"ons\n",
    "def _gaussian_init(output_dim, input_dim):\n",
    "    W, B = np.random.randn(output_dim,input_dim), np.random.randn(output_dim,1)\n",
    "    return W,B\n",
    "def _xavier_init(output_dim,input_dim):\n",
    "    W, B = np.random.uniform(-np.sqrt(6)/(np.sqrt(output_dim+input_dim)),np.sqrt(6)/(np.sqrt(output_dim+input_dim)),[output_dim,input_dim]), np.random.uniform(-np.sqrt(6)/(np.sqrt(output_dim+1)),np.sqrt(6)/(np.sqrt(output_dim+1)),[output_dim,1])\n",
    "    return W,B\n",
    "#add relu     \n",
    "\n",
    "#forward props\n",
    "def _sigmoid(x):\n",
    "    return (1/(1+np.exp(-x)))\n",
    "def _sigmoid_fp(prev_a, W, B):\n",
    "    Z = W @ prev_a + B\n",
    "    return Z, _sigmoid(Z)    \n",
    "def _relu(x):\n",
    "    return(max(0,x))\n",
    "def _relu_fp(prev_a, W, B):\n",
    "    Z = W @ prev_a + B\n",
    "    return Z, _relu(Z)\n",
    "def _last_leg_softmax(previous_a, W, B):\n",
    "    Z = W @ previous_a + B\n",
    "    A = np.exp(Z)\n",
    "    A = A / np.sum(A,axis=0) \n",
    "    return Z, A   \n",
    "\n",
    "## BACK PROPOGATION \n",
    "def back_prop_from_z_to_z_sigmoid(dC_dZn, final_layer_or_not,prev_Z, prev_A,W):    \n",
    "\n",
    "    #get diffs at that layer\n",
    "    dZn_dWn = prev_A\n",
    "    dZn_dBn = np.zeros_like(dC_dZn) + 1 #output feature x training examples\n",
    "    #combien to get useful differentials\n",
    "    dC_dWn = dC_dZn @ dZn_dWn.T / dC_dZn.shape[1] #training examples\n",
    "    dC_dBn = np.sum(dC_dZn @ dZn_dBn.T,axis=1,keepdims=True) / dZn_dBn.size ###THIS COULD BE WRONG, MIGHT BE SHAPE. let's try both. BUT IT LOOKS like this is creating a column created from mxmx10, which is shape.\n",
    "\n",
    "    #and then get the previous layer cost fuctions\n",
    "    if final_layer_or_not == 'no':\n",
    "        dZn_dA_minus_one = W\n",
    "        dC__dA_minus_one = (dC_dZn.T @ dZn_dA_minus_one).T\n",
    "        dAn_minus_one__dZ_minus_one = np.exp(-prev_Z) / ((1+np.exp(-prev_Z))**2) #sigmoid differentiated\n",
    "        dC__dZ_minus_one = dC__dA_minus_one * dAn_minus_one__dZ_minus_one\n",
    "        return dC_dWn, dC_dBn, dC__dZ_minus_one\n",
    "    else:\n",
    "        return dC_dWn, dC_dBn, 0"
   ]
  },
  {
   "source": [
    "Unit test 1 - check whether xavier / He gives the right mean and variance of Z1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[784, 10, 10, 10]\n"
     ]
    }
   ],
   "source": [
    "nn_test = Neural_Net_Softmax(784, 10)\n",
    "nn_test.create_layers([10,10], 'relu')\n",
    "nn_test.init_weights()\n",
    "nn_test.forward_prop(x_train_flat,one_hot_y_train)\n",
    "nn_test.back_prop(x_train_flat, one_hot_y_train)\n",
    "print(nn_test.all_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_test.back_prop()"
   ]
  }
 ]
}